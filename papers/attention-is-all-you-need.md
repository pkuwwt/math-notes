
# Attention Is All You Need

  * Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
  * Year: 2017
  * Keywords: sequence transduction, RNN, CNN, encoder-decoder, attention, Transformer
  * Abstract: Traditionally, sequence transduction = RNN/CNN + encoder-decoder + attention. A new architecture Transformer only relys on attention, with better quality and being more parallelizable but requiring less training time.


## Introduction

## Background

## Model Architecture

### Encoder and Decoder Stacks

### Attention

### Position-wise Feed-Forward Networks

### Embeddings and Softmax

### Positional Encoding

## Traning

### Training Data and Batching

### Hardware and Schedule

### Optimizer

### Regularization

## Results

### Machine Translation

### Model Variations

### English Constituency Parsing

## Conclusion

